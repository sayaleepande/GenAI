{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayaleepande/GenAI/blob/main/M5_Lab1_LangChain_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b23c44",
      "metadata": {
        "id": "51b23c44"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">🧠 LangChain Lab 1: Introduction</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">Welcome to the first hands-on lab for LangChain! In this lab, we will:</p>\n",
        "  <p style=\"margin-top: 10px; font-size: 18px; font-weight: bold;\">Instructor: Dr. Dehghani</p>\n",
        "  <p style=\"margin-top: 5px; font-size: 14px;\">\n",
        "    Learn more at <a href=\"https://langchain.com\" target=\"_blank\" style=\"color: #ffdd57; text-decoration: underline;\">LangChain Website</a>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 20px; border: 1px solid #0055d4;\">\n",
        "  <p style=\"line-height: 1.6; font-size: 16px; margin-bottom: 15px;\">\n",
        "    LangChain is a framework for building applications powered by large language models (LLMs).  \n",
        "    It provides modular components—such as prompt templates, memory, and chains—that make it easy to develop, test, and deploy LLM-based solutions.\n",
        "  </p>\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">Lab Objectives</h2>\n",
        "  <ul style=\"line-height: 1.8; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li>Set up LangChain in Google Colab</li>\n",
        "    <li>Interact with OpenAI models</li>\n",
        "    <li>Work with open-source models like Falcon</li>\n",
        "  </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c7683f7f",
      "metadata": {
        "id": "c7683f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f985c8-7640-421f-d804-b16b6b2f55a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m2.1/2.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.4/438.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ⚙️ Install only the essential packages for LangChain with OpenAI & Gemini support\n",
        "\n",
        "!pip install -q --upgrade langchain                # Core LangChain framework for building LLM workflows\n",
        "!pip install -q --upgrade langchain-community      # Community integrations (still useful for many non-OpenAI/Gemini models)\n",
        "!pip install -q --upgrade langchain-openai         # ✅ NEW: Dedicated package for OpenAI integrations\n",
        "!pip install -q --upgrade langchain-google-genai   # Integration for Google's Gemini models\n",
        "!pip install -q --upgrade openai                   # OpenAI SDK for native API calls (not strictly needed for LangChain, but often useful)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6843a5c",
      "metadata": {
        "id": "c6843a5c"
      },
      "source": [
        "## 🔑 Step 2: Set Up OpenAI API Key\n",
        "If you want to use OpenAI models like GPT-4, you need an API key. Run the code below and enter your key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fba2e457",
      "metadata": {
        "id": "fba2e457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e50958-d2c6-47a5-d6b2-08907ec3bbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAI API key loaded successfully!\n",
            "✅ Google Gemini API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# ⚙️ Load API Keys from Colab Secrets\n",
        "# ==================================\n",
        "\n",
        "import os                                  # Used to set environment variables for API keys\n",
        "from google.colab import userdata          # To securely access stored secrets in Colab\n",
        "\n",
        "# Retrieve your stored secrets (API keys)\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')   # OpenAI API key for GPT models\n",
        "GEMINI_API_KEY = userdata.get('Gemini_api_key')   # Google Gemini API key for Gemini models\n",
        "\n",
        "# Set environment variables for the APIs and confirm success\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY   # Set OpenAI key as environment variable\n",
        "    print(\"✅ OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"❌ OpenAI API key not found. Please set 'OPENAI_API_KEY' in Colab secrets.\")\n",
        "\n",
        "if GEMINI_API_KEY:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY   # Set Gemini key as environment variable\n",
        "    print(\"✅ Google Gemini API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"❌ Google Gemini API key not found. Please set 'GEMINI_API_KEY' in Colab secrets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27zoTSlqMFr",
      "metadata": {
        "id": "e27zoTSlqMFr"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; text-align: center; border-radius: 10px; margin-bottom: 20px;\">\n",
        "  <h1 style=\"font-size: 32px; margin-bottom: 10px;\">🤖 LLM Connection Check (via LangChain)</h1>\n",
        "  <p style=\"margin: 0; font-size: 16px;\">\n",
        "    This step tests if <strong>LangChain</strong> can communicate with your selected language model—such as OpenAI, Gemini, or others.<br>\n",
        "    All messaging with the LLM is handled by LangChain, giving you a flexible and unified workflow.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 18px; margin-bottom: 22px; border: 1px solid #0055d4;\">\n",
        "  <ol style=\"line-height: 1.7; font-size: 16px; margin: 0; padding-left: 20px;\">\n",
        "    <li><strong>We call the language model through LangChain</strong> — not directly. LangChain manages the connection and delivers the response. 🌐</li>\n",
        "    <li>If you get a valid reply, your setup is good to go for any LLM provider! 🎯</li>\n",
        "  </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; border-radius: 12px; padding: 18px; border: 1px solid #0055d4; margin-bottom: 22px;\">\n",
        "  <h2 style=\"color: #0055d4; font-size: 22px; margin-top: 0; margin-bottom: 10px;\">🔹 How <code>.invoke()</code> Works</h2>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 12px;\">\n",
        "    The <code>.invoke()</code> method is the main way to send prompts (messages) to a language model in LangChain.<br>\n",
        "    You wrap your input in a <code>HumanMessage</code> (or <code>SystemMessage</code>, etc.), then pass it as a list.<br>\n",
        "    LangChain sends the message to the LLM and returns a response object.\n",
        "  </p>\n",
        "  <pre style=\"background: #f7faff; border-radius: 8px; padding: 12px; font-size: 15px; border: 1px solid #cce0ff;\">\n",
        "response = llm.invoke([HumanMessage(content=\"Summarize LangChain in one sentence.\")])\n",
        "print(response.content)  # ⬅️ This gives you the model's reply\n",
        "  </pre>\n",
        "  <p style=\"font-size: 15px; margin: 0;\">\n",
        "    <strong>Tip:</strong> This workflow is the same for all LLM providers supported by LangChain!\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "HpUBxiUZqLqU",
      "metadata": {
        "id": "HpUBxiUZqLqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7364c935-3d74-49bc-efa5-6c6d8e2be9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 OpenAI Response: LangChain is a decentralized platform for language learning using blockchain technology.\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 🌟 LangChain Connection Test: OpenAI (Generalized)\n",
        "# =========================================================\n",
        "\n",
        "# 🚩 Check available OpenAI models at: https://platform.openai.com/docs/models/\n",
        "#   (Recommended: 'gpt-3.5-turbo', 'gpt-4o', 'gpt-4-turbo', etc.)\n",
        "\n",
        "# 1️⃣ Import the correct ChatOpenAI class (from the updated package!)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# 2️⃣ Set your desired model (update to any available OpenAI chat model as needed)\n",
        "OPENAI_MODEL = \"gpt-3.5-turbo\"    # <-- Change this string to any OpenAI chat model you have access to\n",
        "\n",
        "# 3️⃣ Initialize the LLM connection\n",
        "llm = ChatOpenAI(\n",
        "    model=OPENAI_MODEL,\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# 4️⃣ Define your prompt\n",
        "prompt = \"What is LangChain in one short sentence?\"\n",
        "\n",
        "# 5️⃣ Invoke the model (send the prompt as a HumanMessage)\n",
        "response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "# 6️⃣ Display the response\n",
        "print(\"🔹 OpenAI Response:\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================\n",
        "# 🧐 Inspecting the LLM Response Object\n",
        "# =======================================\n",
        "\n",
        "# 🏷️ Print the type of the response object\n",
        "print(\"🔖 Type of response:\", type(response))\n",
        "\n",
        "# 🧩 Print the raw response object (see all attributes/methods)\n",
        "print(\"\\n🔎 Raw response object:\\n\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "_81DG3631GhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3c4977-15f0-441b-c4cc-86a12dd791c7"
      },
      "id": "_81DG3631GhY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔖 Type of response: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "\n",
            "🔎 Raw response object:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is a decentralized platform for language learning using blockchain technology.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 16, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbvMaXhFliUqMcBWBPnqoyXJjSGrf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1721cd85-4f15-4c18-9cc9-bbfb88e88fbe-0', usage_metadata={'input_tokens': 16, 'output_tokens': 13, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "49B7fPC_sM6g",
      "metadata": {
        "id": "49B7fPC_sM6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05f9a0e-026e-4fd3-8434-ebde04e09fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main response text: LangChain is a decentralized platform for language learning using blockchain technology.\n",
            "Number of prompt tokens used: 16\n",
            "Number of response tokens generated: 13\n"
          ]
        }
      ],
      "source": [
        "# ✋ **Hands-On: Reading the LLM's Response**\n",
        "# ============================================\n",
        "\n",
        "# Replace '-----' in the placeholders with the correct method or key to retrieve the requested information.\n",
        "\n",
        "# Task 1: Get the main content from the LLM response\n",
        "response_text = response.content  # Extract the main response text (e.g., \"content\" for ChatOpenAI)\n",
        "print(\"Main response text:\", response_text)\n",
        "\n",
        "# Task 2: Get the number of prompt tokens used\n",
        "prompt_tokens = response.usage_metadata['input_tokens']  # Extract the number of tokens used in the input prompt\n",
        "print(\"Number of prompt tokens used:\",prompt_tokens )\n",
        "\n",
        "# Task 3: Get the number of response tokens generated\n",
        "response_tokens = response.usage_metadata['output_tokens']  # Extract the number of tokens used in the generated response\n",
        "print(\"Number of response tokens generated:\", response_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ng1KX4E_xXqH",
      "metadata": {
        "id": "Ng1KX4E_xXqH"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 22px 28px 16px 28px; border-radius: 12px; margin-bottom: 18px; text-align: center;\">\n",
        "  <h2 style=\"font-size: 26px; margin-bottom: 8px;\">💬 Multi-Turn Conversation in LangChain</h2>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 0;\">\n",
        "    A <strong>multi-turn conversation</strong> allows an AI to retain context across multiple exchanges, making interactions more natural and intelligent.<br>\n",
        "    Instead of treating each query independently, the AI builds on previous inputs, improving coherence and accuracy. <span style=\"font-size: 22px;\">🤝</span>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 12px; padding: 18px; margin-bottom: 22px; border: 1px solid #0055d4;\">\n",
        "  <h3 style=\"color: #0055d4; font-size: 20px; margin-top: 0;\">✨ Why Use Multi-Turn Conversations?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0; padding-left: 20px; line-height: 1.7;\">\n",
        "    <li><strong>Context Retention</strong> – AI remembers past interactions, leading to more relevant responses.</li>\n",
        "    <li><strong>Realistic Dialogue</strong> – Mimics human conversations, making chatbots more engaging.</li>\n",
        "    <li><strong>Improved Accuracy</strong> – Responses are refined based on earlier exchanges.</li>\n",
        "    <li><strong>Scalable Design</strong> – Supports long-form discussions without losing context.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15px; margin-top: 12px;\">\n",
        "    <em>This approach is ideal for applications like <b>financial advisors, chatbots, research assistants</b>, and other AI-driven services that require ongoing, dynamic conversations.</em>\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "VPScknqNr28z",
      "metadata": {
        "id": "VPScknqNr28z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d608ed-0197-4361-b248-ee8c2abd0cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coffee reached Europe in the 17th century, with the first coffeehouse opening in Venice in 1645.\n"
          ]
        }
      ],
      "source": [
        "# 🌟 Multi-Turn Conversation Example: Coffee History (Concise Responses)\n",
        "# =======================================================================\n",
        "\n",
        "# Import message classes: SystemMessage (AI behavior), HumanMessage (user input), AIMessage (AI responses)\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Instruct the AI to act as a coffee historian and reply in one short sentence\n",
        "messages = [\n",
        "   SystemMessage(content=\"You are a coffee historian. Provide concise, one-sentence answers.\"),\n",
        "   HumanMessage(content=\"What is the origin of coffee?\"),\n",
        "   AIMessage(content=\"Coffee was first discovered in the Kaffa region of Ethiopia in the 9th century.\"),\n",
        "   HumanMessage(content=\"How did coffee spread beyond Ethiopia?\"),\n",
        "   AIMessage(content=\"It traveled via trade routes into Yemen and then throughout the Ottoman Empire.\"),\n",
        "   HumanMessage(content=\"When did coffee reach Europe?\")\n",
        "]\n",
        "\n",
        "# Send the structured messages to your initialized LangChain model\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)  # Expect a one-sentence response on coffee reaching Europe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "jhpvSXq7yCv_",
      "metadata": {
        "id": "jhpvSXq7yCv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4813a37-06bb-460a-c602-1dadd4d1c59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kyoto is a perfect choice for a springtime visit, as it is known for its beautiful cherry blossoms and traditional Japanese culture. With its historic temples, serene gardens, and picturesque streets, Kyoto offers a rich cultural experience amidst the vibrant colors of spring.\n"
          ]
        }
      ],
      "source": [
        "# ✋ **Hands-On: Completing a Multi-Turn Conversation (Travel Assistant)**\n",
        "#==========================================================================\n",
        "\n",
        "# 📌 Task Instructions:\n",
        "# - Below is a conversation with a **travel assistant** AI.\n",
        "# - Fill in the last `HumanMessage` with a relevant travel-related question.\n",
        "# - Complete the placeholder `response = ----(----)` to correctly call the LLM.\n",
        "\n",
        "# Define a conversation with missing parts\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful travel assistant, providing recommendations for destinations and travel tips.\"),\n",
        "    HumanMessage(content=\"What are some must-visit places in Japan?\"),\n",
        "    AIMessage(content=\"Some must-visit places in Japan include Tokyo, Kyoto, and Osaka. Each city offers unique cultural and historical experiences.\"),\n",
        "    HumanMessage(content=\"Suggest a city to travel during Spring season which has history and culture\")  #  Task: Fill in a relevant follow-up question\n",
        "]\n",
        "\n",
        "# 🔧 Task: Complete the function to generate a response from the LLM\n",
        "response = llm.invoke(messages)  # Call the LLM correctly using the messages\n",
        "\n",
        "print(response.content)  # Display the AI's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The AI response seems correct. It was great to see it mention the city would be good during the springtime too (the context to which was included in the HumanMessage).."
      ],
      "metadata": {
        "id": "CWlleSK_pBFq"
      },
      "id": "CWlleSK_pBFq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 18px;\"> <h2 style=\"margin: 0 0 10px 0;\">🔹 What is a Multi-LLM Model?</h2> <p style=\"margin: 0;\"> A <strong>Multi-LLM model</strong> is a setup where you can send the same question or conversation to multiple large language models (LLMs)—for example, OpenAI's GPT-4 and Google's Gemini—at the same time. This allows you to compare their answers, pick the best response, or even blend their strengths for more reliable results. </p> </div> <div style=\"background: #f0f5ff; border-radius: 12px; padding: 16px; border: 1px solid #0055d4;\"> <h3 style=\"color: #0055d4; margin: 0 0 8px 0;\">Why Does LangChain Make This Easy?</h3> <ul style=\"padding-left: 20px; margin: 0; font-size: 16px;\"> <li><b>Unified Interface:</b> LangChain lets you connect to many LLM providers (OpenAI, Google, Hugging Face, and more) using the same simple code.</li> <li><b>Modular Chaining:</b> You can send the same messages or workflows to any model without rewriting your logic for each one.</li> <li><b>Rapid Experimentation:</b> Instantly compare outputs, performance, or reliability from different models—helping you choose the right LLM for your app or research.</li> </ul> </div> <div style=\"background: #fff3f3; border-radius: 12px; padding: 12px; border: 1px solid #ff6b6b; color: #d32f2f;\"> <b>In short:</b> <i>LangChain empowers you to use, compare, and combine multiple AI models in one unified workflow—making your LLM projects more flexible and future-proof.</i> </div>"
      ],
      "metadata": {
        "id": "Kg0J6tUbHOk5"
      },
      "id": "Kg0J6tUbHOk5"
    },
    {
      "cell_type": "code",
      "source": [
        "# ✨ Multi-LLM Comparison: Gemini vs. ChatGPT (Coffee in 2050)\n",
        "# ==========================================================================\n",
        "\n",
        "# 📝 Imports explained:\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage   # For chat message formatting (system/user/AI)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI             # Google's Gemini LLM via LangChain\n",
        "from langchain.chat_models import ChatOpenAI                          # OpenAI models (ChatGPT, GPT-4) via LangChain\n",
        "import pandas as pd                                                   # For tabular storage and display\n",
        "\n",
        "# 🟦 System and User Setup: Future Coffee Culture\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a fun futurist AI barista. Predict and explain what coffee culture will look like in the year 2050.\"),\n",
        "    HumanMessage(content=\"Describe the biggest change in how people enjoy coffee in 2050 in few sentnene.\"),\n",
        "]\n",
        "\n",
        "# ☕ Initialize LLMs (update model names as needed)\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7)\n",
        "chatgpt_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# 🤖 Generate responses from both models\n",
        "gemini_response = gemini_llm.invoke(messages)\n",
        "chatgpt_response = chatgpt_llm.invoke(messages)\n",
        "\n",
        "# 📊 Store responses in a DataFrame for easy comparison and future use\n",
        "df = pd.DataFrame({\n",
        "    \"Model\": [\"Gemini\", \"ChatGPT\"],\n",
        "    \"Response\": [gemini_response.content, chatgpt_response.content]\n",
        "})\n",
        "\n",
        "# 🖨️ Print the DataFrame as a table (plain text)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "xBAmR3BNGzqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "cc614a8a-ee71-4acf-f435-e0cf3493533c"
      },
      "id": "xBAmR3BNGzqZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Model  \\\n",
              "0   Gemini   \n",
              "1  ChatGPT   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Response  \n",
              "0                                                                                    Forget your morning latte run! In 2050, personalized, on-demand coffee is the norm.  Hyper-personalized brews, tailored to your biometrics and mood via smart-home systems and delivered instantly via a sleek, countertop coffee-synthesizer, will replace the traditional cafe experience for many.  Sustainability will be paramount, with hyperlocal, lab-grown beans and zero-waste brewing processes.  \n",
              "1  In 2050, coffee culture has been revolutionized with the advent of the \"Personalized Coffee Experience.\" This innovation uses AI and advanced biotechnology to tailor each coffee blend to an individual's taste preferences, health requirements, and even mood states. More than just a beverage, coffee has become an immersive, multi-sensory experience, integrating elements of augmented reality to add visual appeal, personalized narratives, and even virtual coffee tasting tours.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65198fff-2746-470d-b390-4054e7ab3a15\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gemini</td>\n",
              "      <td>Forget your morning latte run! In 2050, personalized, on-demand coffee is the norm.  Hyper-personalized brews, tailored to your biometrics and mood via smart-home systems and delivered instantly via a sleek, countertop coffee-synthesizer, will replace the traditional cafe experience for many.  Sustainability will be paramount, with hyperlocal, lab-grown beans and zero-waste brewing processes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ChatGPT</td>\n",
              "      <td>In 2050, coffee culture has been revolutionized with the advent of the \"Personalized Coffee Experience.\" This innovation uses AI and advanced biotechnology to tailor each coffee blend to an individual's taste preferences, health requirements, and even mood states. More than just a beverage, coffee has become an immersive, multi-sensory experience, integrating elements of augmented reality to add visual appeal, personalized narratives, and even virtual coffee tasting tours.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65198fff-2746-470d-b390-4054e7ab3a15')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65198fff-2746-470d-b390-4054e7ab3a15 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65198fff-2746-470d-b390-4054e7ab3a15');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f9d20b3b-fb48-4eed-a009-306be7cc3795\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f9d20b3b-fb48-4eed-a009-306be7cc3795')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f9d20b3b-fb48-4eed-a009-306be7cc3795 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9aa1bb91-43d9-4724-994a-ecf18327d0a4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9aa1bb91-43d9-4724-994a-ecf18327d0a4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ChatGPT\",\n          \"Gemini\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"In 2050, coffee culture has been revolutionized with the advent of the \\\"Personalized Coffee Experience.\\\" This innovation uses AI and advanced biotechnology to tailor each coffee blend to an individual's taste preferences, health requirements, and even mood states. More than just a beverage, coffee has become an immersive, multi-sensory experience, integrating elements of augmented reality to add visual appeal, personalized narratives, and even virtual coffee tasting tours.\",\n          \"Forget your morning latte run! In 2050, personalized, on-demand coffee is the norm.  Hyper-personalized brews, tailored to your biometrics and mood via smart-home systems and delivered instantly via a sleek, countertop coffee-synthesizer, will replace the traditional cafe experience for many.  Sustainability will be paramount, with hyperlocal, lab-grown beans and zero-waste brewing processes.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 📊 LLM Math Reasoning Challenge: Gemini vs. ChatGPT (Lab Style)\n",
        "# =============================================================\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt   # For plotting response times\n",
        "import pandas as pd               # For tabular storage and display\n",
        "\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 📝 Math Problem Prompt: Can LLMs Reason Like Mathematicians?\n",
        "# -------------------------------------------------------------\n",
        "prompt = (\n",
        "    \"Solve for x in the equation: 3x^3 + 5x^2 - 12x + 7 = 21. \"\n",
        "    \"Highlight answers by ---[ x1=  ] ---\"\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 🤖 Initialize LLMs: Gemini & ChatGPT (Low Temp for Accuracy)\n",
        "# -------------------------------------------------------------\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3)\n",
        "chatgpt_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# ⏱️ Get LLM Responses & Measure Response Time\n",
        "# -------------------------------------------------------------\n",
        "start_gemini = time.time()\n",
        "gemini_response = gemini_llm.invoke([HumanMessage(content=prompt)])\n",
        "end_gemini = time.time()\n",
        "gemini_time = end_gemini - start_gemini\n",
        "\n",
        "start_chatgpt = time.time()\n",
        "chatgpt_response = chatgpt_llm.invoke([HumanMessage(content=prompt)])\n",
        "end_chatgpt = time.time()\n",
        "chatgpt_time = end_chatgpt - start_chatgpt\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 📋 Store Model Outputs & Timing in a Table\n",
        "# -------------------------------------------------------------\n",
        "responses = pd.DataFrame({\n",
        "    \"Model\": [\"Gemini\", \"ChatGPT\"],\n",
        "    \"Response\": [gemini_response.content, chatgpt_response.content],\n",
        "    \"Response Time (s)\": [gemini_time, chatgpt_time]\n",
        "})\n",
        "\n",
        "# 🖥️ Display table for easy side-by-side review\n",
        "display(responses)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 📈 Visualize: Response Time Comparison (Brand Colors)\n",
        "# -------------------------------------------------------------\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(\n",
        "    responses[\"Model\"],\n",
        "    responses[\"Response Time (s)\"],\n",
        "    color=[\"#00E3FF\", \"#10A37F\"]  # Gemini blue, OpenAI green\n",
        ")\n",
        "plt.ylabel(\"Response Time (seconds)\", fontsize=12)\n",
        "plt.title(\"LLM Math Problem Response Time\", fontsize=14, color=\"#0055d4\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 🧐 Notes & Analysis:\n",
        "#   - The equation 3x^3 + 5x^2 - 12x + 7 = 21 has three real solutions:\n",
        "#         x = -1,   x ≈ 1.852,   x ≈ -2.519\n",
        "#   - Review the 'Response' column above: Does the LLM show clear reasoning?\n",
        "#   - Did the model provide a step-by-step solution, not just the final answer?\n",
        "# -------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "zG6lB7FONaGG"
      },
      "id": "zG6lB7FONaGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 🧑‍💻 Hands-On: Exploring LLM Prompt Engineering & Model Choice\n",
        "# ============================================================\n",
        "\n",
        "\"\"\"\n",
        "Task:\n",
        "1. Choose a different math equation (e.g., a different cubic, a system of equations, or even a word problem).\n",
        "2. Experiment with your prompt style! For example:\n",
        "    - Ask the LLM to \"show every step\"\n",
        "    - Use ReAct: \"First, reflect on what is needed, then solve, then verify your solution\"\n",
        "    - Ask for self-evaluation: \"Explain why your answer makes sense\"\n",
        "3. Try at least TWO different LLMs (e.g., Gemini, ChatGPT, or another you have access to).\n",
        "4. Record response content and timing in a DataFrame.\n",
        "5. At the end, **write down your observations**:\n",
        "    - Did the prompt style or LLM change the accuracy or reasoning?\n",
        "    - Which model did better? Was ReAct or self-checking effective?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "yPe3I6ldR6WI"
      },
      "id": "yPe3I6ldR6WI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 🔎 Gemini Model Explorer: List Top 10 Newest Models\n",
        "# ============================================================\n",
        "\n",
        "# Import the Gemini model listing function\n",
        "from google.generativeai import list_models\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 🚀 Fetch & Display: Newest Gemini Models (Top 10)\n",
        "# ------------------------------------------------------------\n",
        "gemini_data = []\n",
        "try:\n",
        "    model_list = list(list_models())\n",
        "    # 🗂️ Sort by model name in reverse (often, higher = newer)\n",
        "    model_list_sorted = sorted(model_list, key=lambda m: m.name, reverse=True)\n",
        "    for m in model_list_sorted[:10]:  # Show only the top 10\n",
        "        gemini_data.append({\n",
        "            \"Model Name\": m.name,\n",
        "            \"Supported Methods\": \", \".join(m.supported_generation_methods)\n",
        "        })\n",
        "    df_gemini = pd.DataFrame(gemini_data)\n",
        "\n",
        "    print(\"🔵 === Top 10 Newest Gemini Models ===\")\n",
        "    display(df_gemini)\n",
        "\n",
        "    if not gemini_data:\n",
        "        print(\"⚠️ No Gemini models found. Your API key may not have access, or Gemini is not available in your region/account.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"❌ Error:\", e)\n"
      ],
      "metadata": {
        "id": "oABOMIFcSjbF"
      },
      "id": "oABOMIFcSjbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 🔵 OpenAI Model Explorer: List Newest Models & Metadata\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 🗝️ Load OpenAI API Key from Environment\n",
        "# ------------------------------------------------------------\n",
        "openai_key = os.environ.get(\"OPENAI_API_KEY\", None)\n",
        "model_data = []\n",
        "\n",
        "if openai_key:\n",
        "    try:\n",
        "        # 🤖 Connect to OpenAI and retrieve models\n",
        "        client = OpenAI(api_key=openai_key)\n",
        "        # 📋 Sort models by name/id in reverse (newest first)\n",
        "        models = sorted(client.models.list().data, key=lambda m: m.id, reverse=True)\n",
        "        for m in models[:15]:  # Show up to 15 models\n",
        "            model_data.append({\n",
        "                \"Model Name\": m.id,\n",
        "                \"Owned By\": getattr(m, \"owned_by\", \"\"),\n",
        "                \"Created\": getattr(m, \"created\", \"\"),\n",
        "                \"Object\": getattr(m, \"object\", \"\")\n",
        "                # Note: Purpose/Description not present in API!\n",
        "            })\n",
        "        df = pd.DataFrame(model_data)\n",
        "        print(\"🔵 === Newest OpenAI Models: Metadata ===\")\n",
        "        display(df)\n",
        "        if not model_data:\n",
        "            print(\"⚠️ No OpenAI models found. Your API key may not have access, or your account is limited.\")\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error checking OpenAI API key:\", str(e))\n",
        "else:\n",
        "    print(\"⚠️ No OpenAI API key found in environment.\")\n"
      ],
      "metadata": {
        "id": "U9wnZ102LU9V"
      },
      "id": "U9wnZ102LU9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 24px; border-radius: 12px; margin-bottom: 20px;\">\n",
        "  <h2 style=\"margin-top:0; font-size: 26px;\">🤗 Introduction: Hugging Face & Classic LLMs</h2>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    <strong>Hugging Face</strong> is an open-source platform that hosts thousands of ready-to-use machine learning models, making it easy to experiment with and deploy powerful language models locally or in the cloud.<br>\n",
        "    In this section, we'll compare two popular Hugging Face models:\n",
        "  </p>\n",
        "  <ul style=\"font-size: 16px; margin: 18px 0 8px 18px;\">\n",
        "    <li><span style=\"font-size: 20px;\">🤖</span> <b>GPT-2</b> – One of the first large language models to generate human-like text. It set the stage for the modern LLM boom, and remains a classic benchmark.</li>\n",
        "    <li><span style=\"font-size: 20px;\">⚡</span> <b>DistilGPT2</b> – A smaller, faster, and more efficient version of GPT-2. It offers nearly the same capabilities but with lighter resource requirements, making it ideal for quick prototyping.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15px;\">\n",
        "    We'll see how each model responds to creative prompts and compare their outputs side by side!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "AO3XoMtuOB23"
      },
      "id": "AO3XoMtuOB23"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 🛠️ Setup: Install & Import Required Packages for HF Pipelines\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q --upgrade langchain langchain-huggingface transformers\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline   # Wraps Hugging Face models for LangChain\n",
        "from transformers import pipeline                       # Builds local text-generation pipelines\n",
        "from langchain.schema import HumanMessage               # Formats chat/user messages for LLMs\n"
      ],
      "metadata": {
        "id": "fHxdumvtNrn5"
      },
      "id": "fHxdumvtNrn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 🤗☕ Compare Local Hugging Face LLMs: GPT-2 vs. DistilGPT2\n",
        "# =============================================================\n",
        "\n",
        "import pandas as pd  # For tabular output\n",
        "\n",
        "model_ids = [\n",
        "    (\"🤖 GPT-2\", \"gpt2\"),\n",
        "    (\"⚡ DistilGPT2\", \"distilgpt2\"),\n",
        "]\n",
        "\n",
        "prompt = \"What will be the future of AI in 2050?!\"\n",
        "\n",
        "responses = []\n",
        "\n",
        "for model_name, model_id in model_ids:\n",
        "    # 'pipeline' builds a ready-to-use text-generation model from Hugging Face with a single command\n",
        "    hf_pipe = pipeline(\"text-generation\", model=model_id, max_new_tokens=250)\n",
        "    llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    responses.append({\"Model\": model_name, \"Response\": response.strip()})\n",
        "\n",
        "# Save and display the DataFrame of responses\n",
        "df_response = pd.DataFrame(responses)\n",
        "print(\"🔵 Model responses to the prompt:\")\n",
        "display(df_response)\n"
      ],
      "metadata": {
        "id": "lncdmRGjAXp0"
      },
      "id": "lncdmRGjAXp0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 🧪 Hands-On: Getting Better Answers from DistilGPT2\n",
        "# =============================================================\n",
        "\n",
        "# Background:\n",
        "# DistilGPT2 is a smaller, faster version of GPT-2.\n",
        "# While it's efficient, it often just repeats the prompt or gives very generic answers.\n",
        "# This happens because it isn't specifically trained to follow instructions or answer questions directly.\n",
        "\n",
        "# Your Task:\n",
        "# 1. Try using DistilGPT2 for an open-ended question (e.g., \"What will be the future of AI in 2050?\").\n",
        "# 2. Observe if it repeats the prompt or gives an unhelpful answer.\n",
        "\n",
        "# To improve its output, experiment with these techniques:\n",
        "#   - Make your prompt longer or more explicit (e.g., \"Q: What is the future of AI in 2050?\\nA:\").\n",
        "#   - Add clear instructions or a separator (e.g., \"Answer in 2 sentences: ...\").\n",
        "#   - Enable sampling and set temperature higher (e.g., do_sample=True, temperature=0.8).\n",
        "#   - Try using the full GPT-2 model for comparison.\n",
        "\n",
        "# For each approach:\n",
        "# - Record what happens. Does the model's output improve? When does it still repeat or ignore the prompt?\n",
        "# - Compare your best DistilGPT2 output to GPT-2’s response.\n",
        "# - Summarize your observations and submit them as your exercise report.\n"
      ],
      "metadata": {
        "id": "eYaR5rHMR_4s"
      },
      "id": "eYaR5rHMR_4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; line-height: 1.6; color: #333;\">\n",
        "  <h2 style=\"color: #0055d4; border-bottom: 2px solid #0055d4; padding-bottom: 8px;\">\n",
        "    The Leap from GPT-2 to Today’s Super-Models\n",
        "  </h2>\n",
        "  <p>\n",
        "    Back in 2019, <strong>GPT-2</strong> shook the world with its 1.5 billion parameters (and even a distilled 82 million version) by generating surprisingly coherent paragraphs from web-scraped text. It proved that transformers could model language at scale—but still struggled with long-form reasoning, factual accuracy, and open-ended prompts.\n",
        "  </p>\n",
        "  <p>\n",
        "    Its lighter sibling, <strong>DistilGPT2</strong>, was optimized for speed and efficiency but often echoes the prompt or produces very basic completions, especially for creative or complex questions. This is a common limitation with small, “vanilla” language models that haven’t been tuned for instruction or dialogue.\n",
        "  </p>\n",
        "  <p>\n",
        "    Fast forward to today: modern LLMs boast tens or hundreds of billions of parameters, trained on diverse, multilingual data and fine-tuned with human feedback. They not only write essays and debug code, but also follow instructions, explain reasoning, and adapt to specialized domains. The leap from “sometimes impressive, often generic” to “genuinely useful and insightful” has happened in just a few years—a testament to the rapid evolution of generative AI.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "gqpheBxXLhL7"
      },
      "id": "gqpheBxXLhL7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #f0f5ff 0%, #e6f0ff 100%); border-radius: 12px; padding: 25px; margin-bottom: 25px; border: 1px solid #0055d4;\">\n",
        "  <h2 style=\"color: #0055d4; margin-top: 0; font-size: 26px; padding-bottom: 10px; border-bottom: 2px solid #0055d4;\">🦅 Falcon: High-Performance Open-Source Large Language Model</h2>\n",
        "  <p style=\"font-size: 16px; color: #222; line-height: 1.7; margin-bottom: 18px;\">\n",
        "    <b>Falcon</b> is a state-of-the-art, open-source large language model (LLM) developed by the <a href=\"https://www.tii.ae/\" target=\"_blank\" style=\"color:#0055d4;text-decoration:underline;\"><b>Technology Innovation Institute (TII)</b></a> in Abu Dhabi. Falcon stands out for its high efficiency and performance, and is widely adopted in both research and industry.\n",
        "  </p>\n",
        "  <ul style=\"margin-bottom: 18px; font-size: 16px;\">\n",
        "    <li>🚀 Available in multiple sizes: <b>Falcon-7B</b> and <b>Falcon-40B</b></li>\n",
        "    <li>⚡ Optimized for low memory usage and fast inference</li>\n",
        "    <li>🛠️ Supports a broad range of NLP tasks (text generation, summarization, chat, and more)</li>\n",
        "    <li>🌐 Fully open-source with community support on Hugging Face</li>\n",
        "  </ul>\n",
        "  <div style=\"margin-bottom: 10px; font-size: 16px;\">\n",
        "    <b>Useful Resources:</b>\n",
        "    <ul>\n",
        "      <li><a href=\"https://huggingface.co/tiiuae/falcon-7b\" target=\"_blank\" style=\"color:#0055d4;\">Falcon-7B Model Card (Hugging Face)</a></li>\n",
        "      <li><a href=\"https://github.com/tiiuae/falcon-llm\" target=\"_blank\" style=\"color:#0055d4;\">Falcon GitHub Repository</a></li>\n",
        "    </ul>\n",
        "  </div>\n",
        "  <div style=\"background: #e6f0ff; border-radius: 8px; padding: 14px 18px; margin-top: 16px; border-left: 5px solid #0055d4;\">\n",
        "    <b>Why Falcon?</b> <br>\n",
        "    Falcon models are a powerful choice for building efficient, scalable AI applications, and serve as a strong open-source alternative to proprietary LLMs.\n",
        "  </div>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "GpUYslkAVzK6"
      },
      "id": "GpUYslkAVzK6"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# =============================================================\n",
        "# 🦾 Falcon-7B: Local Model Loading & Pipeline Initialization\n",
        "# =============================================================\n",
        "\n",
        "- Loads the Falcon-7B large language model and tokenizer using Hugging Face `transformers`.\n",
        "- Sets up a text-generation pipeline that runs efficiently on your Colab or local GPU (using bfloat16 precision and auto device selection).\n",
        "- This pipeline lets you generate human-like text locally—no internet or external API needed once the model is downloaded.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Specify the model name (Falcon-7B)\n",
        "model = \"tiiuae/falcon-7b\"\n",
        "\n",
        "# Load the tokenizer for the Falcon-7B model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# Build a text-generation pipeline for efficient local inference\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",           # Task: generate text\n",
        "    model=model,                 # Model to use\n",
        "    tokenizer=tokenizer,         # Tokenizer to use\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16 for memory efficiency (if supported)\n",
        "    device_map=\"auto\",           # Automatically use GPU if available\n",
        ")\n"
      ],
      "metadata": {
        "id": "8RPdlrhzU63V"
      },
      "id": "8RPdlrhzU63V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### ⚡️ How Does This Cell Work?\n",
        ">\n",
        "> - The code imports 🤗 `transformers` and loads the Falcon-7B model directly into your Colab session.\n",
        "> - Downloading and setting up the model **can take several minutes** (due to the large size and initial processing).\n",
        "> - **Falcon-7B is an open-source model**: you don't need any API key, and all text generation happens locally on your Colab GPU—no outside cloud service required!\n"
      ],
      "metadata": {
        "id": "SbAb1p6WXXSk"
      },
      "id": "SbAb1p6WXXSk"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# =============================================================\n",
        "# ☕🦾 Falcon-7B CalmMindBot: AI Multi-Turn Wellness Demo\n",
        "# =============================================================\n",
        "\"\"\"\n",
        "\n",
        "prompt = (\n",
        "    \"CalmMindBot is a compassionate AI assistant who supports people with stress and emotional wellness.\"\n",
        "    \"\\nUser: Hi CalmMindBot, I’ve been feeling overwhelmed lately. What’s a simple way to relax?\"\n",
        "    \"\\nCalmMindBot:\"\n",
        ")\n",
        "\n",
        "sequences = text_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=600,        # Allow for several conversation turns\n",
        "    do_sample=True,\n",
        "    top_k=15,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    full_text = seq['generated_text']\n",
        "    # Split at each role's tag (User: or CalmMindBot:)\n",
        "    import re\n",
        "    dialogue_lines = re.split(r'(User:|CalmMindBot:)', full_text)\n",
        "    # The split leaves ['', 'User:', '...', 'CalmMindBot:', '...', ...]\n",
        "    # So we iterate in steps of 2, skipping any empty initial\n",
        "    for i in range(1, len(dialogue_lines), 2):\n",
        "        speaker = dialogue_lines[i].strip()\n",
        "        content = dialogue_lines[i+1].strip()\n",
        "        if speaker == \"User:\":\n",
        "            print(f\"👤 User: {content}\\n\")\n",
        "        elif speaker == \"CalmMindBot:\":\n",
        "            print(f\"🤖 CalmMindBot: {content}\\n\")\n"
      ],
      "metadata": {
        "id": "tUk6h8yRYt-N"
      },
      "id": "tUk6h8yRYt-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### 🤔 How Does a Transformer Model Like Falcon-7B Generate Text?\n",
        ">\n",
        "> - The `transformers` library loads the Falcon-7B model’s weights into your computer or Colab GPU.\n",
        "> - When you give it a prompt, the model **processes each word (token) step by step**, multiplying learned weights (from its neural network) to predict the most likely next word.\n",
        "> - This process repeats, one token at a time, until the model completes its response—creating answers that seem natural and relevant.\n",
        ">\n",
        "> In short: you provide a starting message, and the model uses its “learned math” (weights) to generate a reply, token by token!\n"
      ],
      "metadata": {
        "id": "pBLVEb4_YB7l"
      },
      "id": "pBLVEb4_YB7l"
    },
    {
      "cell_type": "code",
      "source": [
        "# ✋ **Hands-On: Querying Falcon-7B with LangChain**\n",
        "# Replace the placeholders to:\n",
        "# 1️⃣ Initialize the Hugging Face model correctly.\n",
        "# 2️⃣ Use the correct method to query the model.\n",
        "# 3️⃣ Store the response in the right variable.\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# ✅ Step 1: Initialize the Falcon model\n",
        "llm_falcon = ----- (repo_id=\"tiiuae/falcon-7b-instruct\")  # 🔧 Replace '-----' with the correct class\n",
        "\n",
        "# ✅ Step 2: Define the question\n",
        "question = \"How will AI impact the job market in the next decade?\"\n",
        "\n",
        "# ✅ Step 3: Query the model\n",
        "response_falcon = llm_falcon.----- (question)  # 🔧 Replace '-----' with the correct method\n",
        "\n",
        "# ✅ Display the response\n",
        "print(\"🔹 Falcon-7B Response:\", response_falcon)\n"
      ],
      "metadata": {
        "id": "NcHL_zVXDoA0"
      },
      "id": "NcHL_zVXDoA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎉 Congratulations!\n",
        "\n",
        "You have successfully completed the **Introduction to LangChain** lab. 🚀  \n",
        "We hope you found it insightful and are excited to explore more!  \n",
        "\n",
        "💡 **Next Steps:**  \n",
        "- Try using different LLMs in LangChain.  \n",
        "- Experiment with structured vs. plain text prompts.  \n",
        "- Explore advanced features like memory and chains.\n",
        "\n",
        "Happy Coding! 💻✨  \n"
      ],
      "metadata": {
        "id": "BqzZxacjPK6k"
      },
      "id": "BqzZxacjPK6k"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eu2mJRbHp5b7"
      },
      "id": "eu2mJRbHp5b7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}